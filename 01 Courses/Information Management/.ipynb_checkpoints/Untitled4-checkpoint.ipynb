{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -page PAGE -len LEN [-infinite INFINITE] [-usage USAGE] [-comments COMMENTS]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -page/-p, -len/-l\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elena\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "with open('facebook_credentials.txt') as file:\n",
    "    EMAIL = file.readline().split('\"')[1]\n",
    "    PASSWORD = file.readline().split('\"')[1]\n",
    "\n",
    "\n",
    "def _extract_post_text(item):\n",
    "    actualPosts = item.find_all(attrs={\"data-testid\": \"post_message\"})\n",
    "    text = \"\"\n",
    "    if actualPosts:\n",
    "        for posts in actualPosts:\n",
    "            paragraphs = posts.find_all('p')\n",
    "            text = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                text += paragraphs[index].text\n",
    "    return text\n",
    "\n",
    "\n",
    "def _extract_link(item):\n",
    "    postLinks = item.find_all(class_=\"_6ks\")\n",
    "    link = \"\"\n",
    "    for postLink in postLinks:\n",
    "        link = postLink.find('a').get('href')\n",
    "    return link\n",
    "\n",
    "\n",
    "def _extract_post_id(item):\n",
    "    postIds = item.find_all(class_=\"_5pcq\")\n",
    "    post_id = \"\"\n",
    "    for postId in postIds:\n",
    "        post_id = f\"https://www.facebook.com{postId.get('href')}\"\n",
    "    return post_id\n",
    "\n",
    "\n",
    "def _extract_image(item):\n",
    "    postPictures = item.find_all(class_=\"scaledImageFitWidth img\")\n",
    "    image = \"\"\n",
    "    for postPicture in postPictures:\n",
    "        image = postPicture.get('src')\n",
    "    return image\n",
    "\n",
    "\n",
    "def _extract_shares(item):\n",
    "    postShares = item.find_all(class_=\"_4vn1\")\n",
    "    shares = \"\"\n",
    "    for postShare in postShares:\n",
    "\n",
    "        x = postShare.string\n",
    "        if x is not None:\n",
    "            x = x.split(\">\", 1)\n",
    "            shares = x\n",
    "        else:\n",
    "            shares = \"0\"\n",
    "    return shares\n",
    "\n",
    "\n",
    "def _extract_comments(item):\n",
    "    postComments = item.findAll(\"div\", {\"class\": \"_4eek\"})\n",
    "    comments = dict()\n",
    "    # print(postDict)\n",
    "    for comment in postComments:\n",
    "        if comment.find(class_=\"_6qw4\") is None:\n",
    "            continue\n",
    "\n",
    "        commenter = comment.find(class_=\"_6qw4\").text\n",
    "        comments[commenter] = dict()\n",
    "\n",
    "        comment_text = comment.find(\"span\", class_=\"_3l3x\")\n",
    "\n",
    "        if comment_text is not None:\n",
    "            comments[commenter][\"text\"] = comment_text.text\n",
    "\n",
    "        comment_link = comment.find(class_=\"_ns_\")\n",
    "        if comment_link is not None:\n",
    "            comments[commenter][\"link\"] = comment_link.get(\"href\")\n",
    "\n",
    "        comment_pic = comment.find(class_=\"_2txe\")\n",
    "        if comment_pic is not None:\n",
    "            comments[commenter][\"image\"] = comment_pic.find(class_=\"img\").get(\"src\")\n",
    "\n",
    "        commentList = item.find('ul', {'class': '_7791'})\n",
    "        if commentList:\n",
    "            comments = dict()\n",
    "            comment = commentList.find_all('li')\n",
    "            if comment:\n",
    "                for litag in comment:\n",
    "                    aria = litag.find(\"div\", {\"class\": \"_4eek\"})\n",
    "                    if aria:\n",
    "                        commenter = aria.find(class_=\"_6qw4\").text\n",
    "                        comments[commenter] = dict()\n",
    "                        comment_text = litag.find(\"span\", class_=\"_3l3x\")\n",
    "                        if comment_text:\n",
    "                            comments[commenter][\"text\"] = comment_text.text\n",
    "                            # print(str(litag)+\"\\n\")\n",
    "\n",
    "                        comment_link = litag.find(class_=\"_ns_\")\n",
    "                        if comment_link is not None:\n",
    "                            comments[commenter][\"link\"] = comment_link.get(\"href\")\n",
    "\n",
    "                        comment_pic = litag.find(class_=\"_2txe\")\n",
    "                        if comment_pic is not None:\n",
    "                            comments[commenter][\"image\"] = comment_pic.find(class_=\"img\").get(\"src\")\n",
    "\n",
    "                        repliesList = litag.find(class_=\"_2h2j\")\n",
    "                        if repliesList:\n",
    "                            reply = repliesList.find_all('li')\n",
    "                            if reply:\n",
    "                                comments[commenter]['reply'] = dict()\n",
    "                                for litag2 in reply:\n",
    "                                    aria2 = litag2.find(\"div\", {\"class\": \"_4efk\"})\n",
    "                                    if aria2:\n",
    "                                        replier = aria2.find(class_=\"_6qw4\").text\n",
    "                                        if replier:\n",
    "                                            comments[commenter]['reply'][replier] = dict()\n",
    "\n",
    "                                            reply_text = litag2.find(\"span\", class_=\"_3l3x\")\n",
    "                                            if reply_text:\n",
    "                                                comments[commenter]['reply'][replier][\n",
    "                                                    \"reply_text\"] = reply_text.text\n",
    "\n",
    "                                            r_link = litag2.find(class_=\"_ns_\")\n",
    "                                            if r_link is not None:\n",
    "                                                comments[commenter]['reply'][\"link\"] = r_link.get(\"href\")\n",
    "\n",
    "                                            r_pic = litag2.find(class_=\"_2txe\")\n",
    "                                            if r_pic is not None:\n",
    "                                                comments[commenter]['reply'][\"image\"] = r_pic.find(\n",
    "                                                    class_=\"img\").get(\"src\")\n",
    "    return comments\n",
    "\n",
    "\n",
    "def _extract_reaction(item):\n",
    "    toolBar = item.find_all(attrs={\"role\": \"toolbar\"})\n",
    "\n",
    "    if not toolBar:  # pretty fun\n",
    "        return\n",
    "    reaction = dict()\n",
    "    for toolBar_child in toolBar[0].children:\n",
    "        str = toolBar_child['data-testid']\n",
    "        reaction = str.split(\"UFI2TopReactions/tooltip_\")[1]\n",
    "\n",
    "        reaction[reaction] = 0\n",
    "\n",
    "        for toolBar_child_child in toolBar_child.children:\n",
    "\n",
    "            num = toolBar_child_child['aria-label'].split()[0]\n",
    "\n",
    "            # fix weird ',' happening in some reaction values\n",
    "            num = num.replace(',', '.')\n",
    "\n",
    "            if 'K' in num:\n",
    "                realNum = float(num[:-1]) * 1000\n",
    "            else:\n",
    "                realNum = float(num)\n",
    "\n",
    "            reaction[reaction] = realNum\n",
    "    return reaction\n",
    "\n",
    "\n",
    "def _extract_html(bs_data):\n",
    "\n",
    "    #Add to check\n",
    "    with open('./bs.html',\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(bs_data.prettify()))\n",
    "\n",
    "    k = bs_data.find_all(class_=\"_5pcr userContentWrapper\")\n",
    "    postBigDict = list()\n",
    "\n",
    "    for item in k:\n",
    "        postDict = dict()\n",
    "        postDict['Post'] = _extract_post_text(item)\n",
    "        postDict['Link'] = _extract_link(item)\n",
    "        postDict['PostId'] = _extract_post_id(item)\n",
    "        postDict['Image'] = _extract_image(item)\n",
    "        postDict['Shares'] = _extract_shares(item)\n",
    "        postDict['Comments'] = _extract_comments(item)\n",
    "        # postDict['Reaction'] = _extract_reaction(item)\n",
    "\n",
    "        #Add to check\n",
    "        postBigDict.append(postDict)\n",
    "        with open('./postBigDict.json','w', encoding='utf-8') as file:\n",
    "            file.write(json.dumps(postBigDict, ensure_ascii=False).encode('utf-8').decode())\n",
    "\n",
    "    return postBigDict\n",
    "\n",
    "\n",
    "def _login(browser, email, password):\n",
    "    browser.get(\"http://facebook.com\")\n",
    "    browser.maximize_window()\n",
    "    browser.find_element_by_name(\"email\").send_keys(email)\n",
    "    browser.find_element_by_name(\"pass\").send_keys(password)\n",
    "    browser.find_element_by_id('loginbutton').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "def _count_needed_scrolls(browser, infinite_scroll, numOfPost):\n",
    "    if infinite_scroll:\n",
    "        lenOfPage = browser.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\"\n",
    "        )\n",
    "    else:\n",
    "        # roughly 8 post per scroll kindaOf\n",
    "        lenOfPage = int(numOfPost / 8)\n",
    "    print(\"Number Of Scrolls Needed \" + str(lenOfPage))\n",
    "    return lenOfPage\n",
    "\n",
    "\n",
    "def _scroll(browser, infinite_scroll, lenOfPage):\n",
    "    lastCount = -1\n",
    "    match = False\n",
    "\n",
    "    while not match:\n",
    "        if infinite_scroll:\n",
    "            lastCount = lenOfPage\n",
    "        else:\n",
    "            lastCount += 1\n",
    "\n",
    "        # wait for the browser to load, this time can be changed slightly ~3 seconds with no difference, but 5 seems\n",
    "        # to be stable enough\n",
    "        time.sleep(5)\n",
    "\n",
    "        if infinite_scroll:\n",
    "            lenOfPage = browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "        else:\n",
    "            browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "\n",
    "        if lastCount == lenOfPage:\n",
    "            match = True\n",
    "\n",
    "\n",
    "def extract(page, numOfPost, infinite_scroll=False, scrape_comment=False):\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", {\n",
    "        \"profile.default_content_setting_values.notifications\": 1\n",
    "    })\n",
    "\n",
    "    # chromedriver should be in the same folder as file\n",
    "    browser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n",
    "    _login(browser, EMAIL, PASSWORD)\n",
    "    browser.get(page)\n",
    "    lenOfPage = _count_needed_scrolls(browser, infinite_scroll, numOfPost)\n",
    "    _scroll(browser, infinite_scroll, lenOfPage)\n",
    "\n",
    "    # click on all the comments to scrape them all!\n",
    "    # TODO: need to add more support for additional second level comments\n",
    "    # TODO: ie. comment of a comment\n",
    "\n",
    "    if scrape_comment:\n",
    "        #first uncollapse collapsed comments\n",
    "        unCollapseCommentsButtonsXPath = '//a[contains(@class,\"_666h\")]'\n",
    "        unCollapseCommentsButtons = browser.find_elements_by_xpath(unCollapseCommentsButtonsXPath)\n",
    "        for unCollapseComment in unCollapseCommentsButtons:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass\n",
    "\n",
    "        #second set comment ranking to show all comments\n",
    "        rankDropdowns = browser.find_elements_by_class_name('_2pln') #select boxes who have rank dropdowns\n",
    "        rankXPath = '//div[contains(concat(\" \", @class, \" \"), \"uiContextualLayerPositioner\") and not(contains(concat(\" \", @class, \" \"), \"hidden_elem\"))]//div/ul/li/a[@class=\"_54nc\"]/span/span/div[@data-ordering=\"RANKED_UNFILTERED\"]'\n",
    "        for rankDropdown in rankDropdowns:\n",
    "            #click to open the filter modal\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element_with_offset(rankDropdown, 5, 5)\n",
    "                action.perform()\n",
    "                rankDropdown.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # if modal is opened filter comments\n",
    "            ranked_unfiltered = browser.find_elements_by_xpath(rankXPath) # RANKED_UNFILTERED => (All Comments)\n",
    "            if len(ranked_unfiltered) > 0:\n",
    "                try:\n",
    "                    ranked_unfiltered[0].click()\n",
    "                except:\n",
    "                    pass    \n",
    "        \n",
    "        moreComments = browser.find_elements_by_xpath('//a[@class=\"_4sxc _42ft\"]')\n",
    "        print(\"Scrolling through to click on more comments\")\n",
    "        while len(moreComments) != 0:\n",
    "            for moreComment in moreComments:\n",
    "                action = webdriver.common.action_chains.ActionChains(browser)\n",
    "                try:\n",
    "                    # move to where the comment button is\n",
    "                    action.move_to_element_with_offset(moreComment, 5, 5)\n",
    "                    action.perform()\n",
    "                    moreComment.click()\n",
    "                except:\n",
    "                    # do nothing right here\n",
    "                    pass\n",
    "\n",
    "            moreComments = browser.find_elements_by_xpath('//a[@class=\"_4sxc _42ft\"]')\n",
    "\n",
    "    # Now that the page is fully scrolled, grab the source code.\n",
    "    source_data = browser.page_source\n",
    "\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "    bs_data = bs(source_data, 'html.parser')\n",
    "\n",
    "    postBigDict = _extract_html(bs_data)\n",
    "    browser.close()\n",
    "\n",
    "    return postBigDict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Facebook Page Scraper\")\n",
    "    required_parser = parser.add_argument_group(\"required arguments\")\n",
    "    required_parser.add_argument('-page', '-p', help=\"The Facebook Public Page you want to scrape\", required=True)\n",
    "    required_parser.add_argument('-len', '-l', help=\"Number of Posts you want to scrape\", type=int, required=True)\n",
    "    optional_parser = parser.add_argument_group(\"optional arguments\")\n",
    "    optional_parser.add_argument('-infinite', '-i',\n",
    "                                 help=\"Scroll until the end of the page (1 = infinite) (Default is 0)\", type=int,\n",
    "                                 default=0)\n",
    "    optional_parser.add_argument('-usage', '-u', help=\"What to do with the data: \"\n",
    "                                                      \"Print on Screen (PS), \"\n",
    "                                                      \"Write to Text File (WT) (Default is WT)\", default=\"CSV\")\n",
    "\n",
    "    optional_parser.add_argument('-comments', '-c', help=\"Scrape ALL Comments of Posts (y/n) (Default is n). When \"\n",
    "                                                         \"enabled for pages where there are a lot of comments it can \"\n",
    "                                                         \"take a while\", default=\"No\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    infinite = False\n",
    "    if args.infinite == 1:\n",
    "        infinite = True\n",
    "\n",
    "    scrape_comment = False\n",
    "    if args.comments == 'y':\n",
    "        scrape_comment = True\n",
    "\n",
    "    postBigDict = extract(page=args.page, numOfPost=args.len, infinite_scroll=infinite, scrape_comment=scrape_comment)\n",
    "\n",
    "\n",
    "    #TODO: rewrite parser\n",
    "    if args.usage == \"WT\":\n",
    "        with open('output.txt', 'w') as file:\n",
    "            for post in postBigDict:\n",
    "                file.write(json.dumps(post))  # use json load to recover\n",
    "\n",
    "    elif args.usage == \"CSV\":\n",
    "        with open('data.csv', 'w',) as csvfile:\n",
    "           writer = csv.writer(csvfile)\n",
    "           #writer.writerow(['Post', 'Link', 'Image', 'Comments', 'Reaction'])\n",
    "           writer.writerow(['Post', 'Link', 'Image', 'Comments', 'Shares'])\n",
    "\n",
    "           for post in postBigDict:\n",
    "              writer.writerow([post['Post'], post['Link'],post['Image'], post['Comments'], post['Shares']])\n",
    "              #writer.writerow([post['Post'], post['Link'],post['Image'], post['Comments'], post['Reaction']])\n",
    "\n",
    "    else:\n",
    "        for post in postBigDict:\n",
    "            print(post)\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -page PAGE -len LEN [-infinite INFINITE] [-usage USAGE] [-comments COMMENTS]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -page/-p, -len/-l\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "with open('facebook_credentials.txt') as file:\n",
    "    EMAIL = file.readline().split('\"')[1]\n",
    "    PASSWORD = file.readline().split('\"')[1]\n",
    "\n",
    "\n",
    "def _extract_post_text(item):\n",
    "    actualPosts = item.find_all(attrs={\"data-testid\": \"post_message\"})\n",
    "    text = \"\"\n",
    "    if actualPosts:\n",
    "        for posts in actualPosts:\n",
    "            paragraphs = posts.find_all('p')\n",
    "            text = \"\"\n",
    "            for index in range(0, len(paragraphs)):\n",
    "                text += paragraphs[index].text\n",
    "    return text\n",
    "\n",
    "\n",
    "def _extract_link(item):\n",
    "    postLinks = item.find_all(class_=\"_6ks\")\n",
    "    link = \"\"\n",
    "    for postLink in postLinks:\n",
    "        link = postLink.find('a').get('href')\n",
    "    return link\n",
    "\n",
    "\n",
    "def _extract_post_id(item):\n",
    "    postIds = item.find_all(class_=\"_5pcq\")\n",
    "    post_id = \"\"\n",
    "    for postId in postIds:\n",
    "        post_id = f\"https://www.facebook.com{postId.get('href')}\"\n",
    "    return post_id\n",
    "\n",
    "\n",
    "def _extract_image(item):\n",
    "    postPictures = item.find_all(class_=\"scaledImageFitWidth img\")\n",
    "    image = \"\"\n",
    "    for postPicture in postPictures:\n",
    "        image = postPicture.get('src')\n",
    "    return image\n",
    "\n",
    "\n",
    "def _extract_shares(item):\n",
    "    postShares = item.find_all(class_=\"_4vn1\")\n",
    "    shares = \"\"\n",
    "    for postShare in postShares:\n",
    "\n",
    "        x = postShare.string\n",
    "        if x is not None:\n",
    "            x = x.split(\">\", 1)\n",
    "            shares = x\n",
    "        else:\n",
    "            shares = \"0\"\n",
    "    return shares\n",
    "\n",
    "\n",
    "def _extract_comments(item):\n",
    "    postComments = item.findAll(\"div\", {\"class\": \"_4eek\"})\n",
    "    comments = dict()\n",
    "    # print(postDict)\n",
    "    for comment in postComments:\n",
    "        if comment.find(class_=\"_6qw4\") is None:\n",
    "            continue\n",
    "\n",
    "        commenter = comment.find(class_=\"_6qw4\").text\n",
    "        comments[commenter] = dict()\n",
    "\n",
    "        comment_text = comment.find(\"span\", class_=\"_3l3x\")\n",
    "\n",
    "        if comment_text is not None:\n",
    "            comments[commenter][\"text\"] = comment_text.text\n",
    "\n",
    "        comment_link = comment.find(class_=\"_ns_\")\n",
    "        if comment_link is not None:\n",
    "            comments[commenter][\"link\"] = comment_link.get(\"href\")\n",
    "\n",
    "        comment_pic = comment.find(class_=\"_2txe\")\n",
    "        if comment_pic is not None:\n",
    "            comments[commenter][\"image\"] = comment_pic.find(class_=\"img\").get(\"src\")\n",
    "\n",
    "        commentList = item.find('ul', {'class': '_7791'})\n",
    "        if commentList:\n",
    "            comments = dict()\n",
    "            comment = commentList.find_all('li')\n",
    "            if comment:\n",
    "                for litag in comment:\n",
    "                    aria = litag.find(\"div\", {\"class\": \"_4eek\"})\n",
    "                    if aria:\n",
    "                        commenter = aria.find(class_=\"_6qw4\").text\n",
    "                        comments[commenter] = dict()\n",
    "                        comment_text = litag.find(\"span\", class_=\"_3l3x\")\n",
    "                        if comment_text:\n",
    "                            comments[commenter][\"text\"] = comment_text.text\n",
    "                            # print(str(litag)+\"\\n\")\n",
    "\n",
    "                        comment_link = litag.find(class_=\"_ns_\")\n",
    "                        if comment_link is not None:\n",
    "                            comments[commenter][\"link\"] = comment_link.get(\"href\")\n",
    "\n",
    "                        comment_pic = litag.find(class_=\"_2txe\")\n",
    "                        if comment_pic is not None:\n",
    "                            comments[commenter][\"image\"] = comment_pic.find(class_=\"img\").get(\"src\")\n",
    "\n",
    "                        repliesList = litag.find(class_=\"_2h2j\")\n",
    "                        if repliesList:\n",
    "                            reply = repliesList.find_all('li')\n",
    "                            if reply:\n",
    "                                comments[commenter]['reply'] = dict()\n",
    "                                for litag2 in reply:\n",
    "                                    aria2 = litag2.find(\"div\", {\"class\": \"_4efk\"})\n",
    "                                    if aria2:\n",
    "                                        replier = aria2.find(class_=\"_6qw4\").text\n",
    "                                        if replier:\n",
    "                                            comments[commenter]['reply'][replier] = dict()\n",
    "\n",
    "                                            reply_text = litag2.find(\"span\", class_=\"_3l3x\")\n",
    "                                            if reply_text:\n",
    "                                                comments[commenter]['reply'][replier][\n",
    "                                                    \"reply_text\"] = reply_text.text\n",
    "\n",
    "                                            r_link = litag2.find(class_=\"_ns_\")\n",
    "                                            if r_link is not None:\n",
    "                                                comments[commenter]['reply'][\"link\"] = r_link.get(\"href\")\n",
    "\n",
    "                                            r_pic = litag2.find(class_=\"_2txe\")\n",
    "                                            if r_pic is not None:\n",
    "                                                comments[commenter]['reply'][\"image\"] = r_pic.find(\n",
    "                                                    class_=\"img\").get(\"src\")\n",
    "    return comments\n",
    "\n",
    "\n",
    "def _extract_reaction(item):\n",
    "    toolBar = item.find_all(attrs={\"role\": \"toolbar\"})\n",
    "\n",
    "    if not toolBar:  # pretty fun\n",
    "        return\n",
    "    reaction = dict()\n",
    "    for toolBar_child in toolBar[0].children:\n",
    "        str = toolBar_child['data-testid']\n",
    "        reaction = str.split(\"UFI2TopReactions/tooltip_\")[1]\n",
    "\n",
    "        reaction[reaction] = 0\n",
    "\n",
    "        for toolBar_child_child in toolBar_child.children:\n",
    "\n",
    "            num = toolBar_child_child['aria-label'].split()[0]\n",
    "\n",
    "            # fix weird ',' happening in some reaction values\n",
    "            num = num.replace(',', '.')\n",
    "\n",
    "            if 'K' in num:\n",
    "                realNum = float(num[:-1]) * 1000\n",
    "            else:\n",
    "                realNum = float(num)\n",
    "\n",
    "            reaction[reaction] = realNum\n",
    "    return reaction\n",
    "\n",
    "\n",
    "def _extract_html(bs_data):\n",
    "\n",
    "    #Add to check\n",
    "    with open('./bs.html',\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(bs_data.prettify()))\n",
    "\n",
    "    k = bs_data.find_all(class_=\"_5pcr userContentWrapper\")\n",
    "    postBigDict = list()\n",
    "\n",
    "    for item in k:\n",
    "        postDict = dict()\n",
    "        postDict['Post'] = _extract_post_text(item)\n",
    "        postDict['Link'] = _extract_link(item)\n",
    "        postDict['PostId'] = _extract_post_id(item)\n",
    "        postDict['Image'] = _extract_image(item)\n",
    "        postDict['Shares'] = _extract_shares(item)\n",
    "        postDict['Comments'] = _extract_comments(item)\n",
    "        # postDict['Reaction'] = _extract_reaction(item)\n",
    "\n",
    "        #Add to check\n",
    "        postBigDict.append(postDict)\n",
    "        with open('./postBigDict.json','w', encoding='utf-8') as file:\n",
    "            file.write(json.dumps(postBigDict, ensure_ascii=False).encode('utf-8').decode())\n",
    "\n",
    "    return postBigDict\n",
    "\n",
    "\n",
    "def _login(browser, email, password):\n",
    "    browser.get(\"http://facebook.com\")\n",
    "    browser.maximize_window()\n",
    "    browser.find_element_by_name(\"email\").send_keys(email)\n",
    "    browser.find_element_by_name(\"pass\").send_keys(password)\n",
    "    browser.find_element_by_id('loginbutton').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "def _count_needed_scrolls(browser, infinite_scroll, numOfPost):\n",
    "    if infinite_scroll:\n",
    "        lenOfPage = browser.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\"\n",
    "        )\n",
    "    else:\n",
    "        # roughly 8 post per scroll kindaOf\n",
    "        lenOfPage = int(numOfPost / 8)\n",
    "    print(\"Number Of Scrolls Needed \" + str(lenOfPage))\n",
    "    return lenOfPage\n",
    "\n",
    "\n",
    "def _scroll(browser, infinite_scroll, lenOfPage):\n",
    "    lastCount = -1\n",
    "    match = False\n",
    "\n",
    "    while not match:\n",
    "        if infinite_scroll:\n",
    "            lastCount = lenOfPage\n",
    "        else:\n",
    "            lastCount += 1\n",
    "\n",
    "        # wait for the browser to load, this time can be changed slightly ~3 seconds with no difference, but 5 seems\n",
    "        # to be stable enough\n",
    "        time.sleep(5)\n",
    "\n",
    "        if infinite_scroll:\n",
    "            lenOfPage = browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "        else:\n",
    "            browser.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return \"\n",
    "                \"lenOfPage;\")\n",
    "\n",
    "        if lastCount == lenOfPage:\n",
    "            match = True\n",
    "\n",
    "\n",
    "def extract(page, numOfPost, infinite_scroll=False, scrape_comment=False):\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", {\n",
    "        \"profile.default_content_setting_values.notifications\": 1\n",
    "    })\n",
    "\n",
    "    # chromedriver should be in the same folder as file\n",
    "    browser = webdriver.Chrome(executable_path = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\", options=option)\n",
    "    _login(browser, EMAIL, PASSWORD)\n",
    "    browser.get(page)\n",
    "    lenOfPage = _count_needed_scrolls(browser, infinite_scroll, numOfPost)\n",
    "    _scroll(browser, infinite_scroll, lenOfPage)\n",
    "\n",
    "    # click on all the comments to scrape them all!\n",
    "    # TODO: need to add more support for additional second level comments\n",
    "    # TODO: ie. comment of a comment\n",
    "\n",
    "    if scrape_comment:\n",
    "        #first uncollapse collapsed comments\n",
    "        unCollapseCommentsButtonsXPath = '//a[contains(@class,\"_666h\")]'\n",
    "        unCollapseCommentsButtons = browser.find_elements_by_xpath(unCollapseCommentsButtonsXPath)\n",
    "        for unCollapseComment in unCollapseCommentsButtons:\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                # move to where the un collapse on is\n",
    "                action.move_to_element_with_offset(unCollapseComment, 5, 5)\n",
    "                action.perform()\n",
    "                unCollapseComment.click()\n",
    "            except:\n",
    "                # do nothing right here\n",
    "                pass\n",
    "\n",
    "        #second set comment ranking to show all comments\n",
    "        rankDropdowns = browser.find_elements_by_class_name('_2pln') #select boxes who have rank dropdowns\n",
    "        rankXPath = '//div[contains(concat(\" \", @class, \" \"), \"uiContextualLayerPositioner\") and not(contains(concat(\" \", @class, \" \"), \"hidden_elem\"))]//div/ul/li/a[@class=\"_54nc\"]/span/span/div[@data-ordering=\"RANKED_UNFILTERED\"]'\n",
    "        for rankDropdown in rankDropdowns:\n",
    "            #click to open the filter modal\n",
    "            action = webdriver.common.action_chains.ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element_with_offset(rankDropdown, 5, 5)\n",
    "                action.perform()\n",
    "                rankDropdown.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # if modal is opened filter comments\n",
    "            ranked_unfiltered = browser.find_elements_by_xpath(rankXPath) # RANKED_UNFILTERED => (All Comments)\n",
    "            if len(ranked_unfiltered) > 0:\n",
    "                try:\n",
    "                    ranked_unfiltered[0].click()\n",
    "                except:\n",
    "                    pass    \n",
    "        \n",
    "        moreComments = browser.find_elements_by_xpath('//a[@class=\"_4sxc _42ft\"]')\n",
    "        print(\"Scrolling through to click on more comments\")\n",
    "        while len(moreComments) != 0:\n",
    "            for moreComment in moreComments:\n",
    "                action = webdriver.common.action_chains.ActionChains(browser)\n",
    "                try:\n",
    "                    # move to where the comment button is\n",
    "                    action.move_to_element_with_offset(moreComment, 5, 5)\n",
    "                    action.perform()\n",
    "                    moreComment.click()\n",
    "                except:\n",
    "                    # do nothing right here\n",
    "                    pass\n",
    "\n",
    "            moreComments = browser.find_elements_by_xpath('//a[@class=\"_4sxc _42ft\"]')\n",
    "\n",
    "    # Now that the page is fully scrolled, grab the source code.\n",
    "    source_data = browser.page_source\n",
    "\n",
    "    # Throw your source into BeautifulSoup and start parsing!\n",
    "    bs_data = bs(source_data, 'html.parser')\n",
    "\n",
    "    postBigDict = _extract_html(bs_data)\n",
    "    browser.close()\n",
    "\n",
    "    return postBigDict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Facebook Page Scraper\")\n",
    "    required_parser = parser.add_argument_group(\"required arguments\")\n",
    "    required_parser.add_argument('-page', '-p', help=\"The Facebook Public Page you want to scrape\", required=True)\n",
    "    required_parser.add_argument('-len', '-l', help=\"Number of Posts you want to scrape\", type=int, required=True)\n",
    "    optional_parser = parser.add_argument_group(\"optional arguments\")\n",
    "    optional_parser.add_argument('-infinite', '-i',\n",
    "                                 help=\"Scroll until the end of the page (1 = infinite) (Default is 0)\", type=int,\n",
    "                                 default=0)\n",
    "    optional_parser.add_argument('-usage', '-u', help=\"What to do with the data: \"\n",
    "                                                      \"Print on Screen (PS), \"\n",
    "                                                      \"Write to Text File (WT) (Default is WT)\", default=\"CSV\")\n",
    "\n",
    "    optional_parser.add_argument('-comments', '-c', help=\"Scrape ALL Comments of Posts (y/n) (Default is n). When \"\n",
    "                                                         \"enabled for pages where there are a lot of comments it can \"\n",
    "                                                         \"take a while\", default=\"No\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    infinite = False\n",
    "    if args.infinite == 1:\n",
    "        infinite = True\n",
    "\n",
    "    scrape_comment = False\n",
    "    if args.comments == 'y':\n",
    "        scrape_comment = True\n",
    "\n",
    "    postBigDict = extract(page=args.page, numOfPost=args.len, infinite_scroll=infinite, scrape_comment=scrape_comment)\n",
    "\n",
    "\n",
    "    #TODO: rewrite parser\n",
    "    if args.usage == \"WT\":\n",
    "        with open('output.txt', 'w') as file:\n",
    "            for post in postBigDict:\n",
    "                file.write(json.dumps(post))  # use json load to recover\n",
    "\n",
    "    elif args.usage == \"CSV\":\n",
    "        with open('data.csv', 'w',) as csvfile:\n",
    "           writer = csv.writer(csvfile)\n",
    "           #writer.writerow(['Post', 'Link', 'Image', 'Comments', 'Reaction'])\n",
    "           writer.writerow(['Post', 'Link', 'Image', 'Comments', 'Shares'])\n",
    "\n",
    "           for post in postBigDict:\n",
    "              writer.writerow([post['Post'], post['Link'],post['Image'], post['Comments'], post['Shares']])\n",
    "              #writer.writerow([post['Post'], post['Link'],post['Image'], post['Comments'], post['Reaction']])\n",
    "\n",
    "    else:\n",
    "        for post in postBigDict:\n",
    "            print(post)\n",
    "\n",
    "    print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-302cec407d48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    344\u001b[0m                                                          \u001b[1;34m\"enabled for pages where there are a lot of comments it can \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                                                          \"take a while\", default=\"No\")\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[0minfinite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1766\u001b[0m     \u001b[1;31m# =====================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1798\u001b[0m         \u001b[1;31m# parse the arguments and exit if there are any errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1800\u001b[1;33m             \u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequired_actions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2034\u001b[1;33m             self.error(_('the following arguments are required: %s') %\n\u001b[0m\u001b[0;32m   2035\u001b[0m                        ', '.join(required_actions))\n\u001b[0;32m   2036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2519\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2521\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
